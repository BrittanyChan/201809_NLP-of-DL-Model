{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --*-- encoding:utf-8 --*--\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "def loadfile():\n",
    "    neg = pd.read_excel('./neg1.xls', header=None, index=None)\n",
    "    pos = pd.read_excel('./pos1.xls', header=None, index=None)\n",
    "    #merge all data\n",
    "    neg = np.array(neg[0])\n",
    "    pos = np.array(pos[0])\n",
    "    return neg,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = loadfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating set of disused words\n",
    "def getstopword(stopwordPath):\n",
    "    stoplist = set()\n",
    "    for line in stopwordPath:\n",
    "        stoplist.add(line.strip())\n",
    "        # print line.strip()\n",
    "    return stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the sentence and remove the disused words\n",
    "def wordsege(text):\n",
    "    # get disused words set\n",
    "    stopwordPath = open('./stopwords(ch).txt', 'r')\n",
    "    stoplist = getstopword(stopwordPath)\n",
    "    stopwordPath.close()\n",
    "\n",
    "    # divide the sentence and remove the disused words with jieba,return list\n",
    "    text_list = []\n",
    "    for document in text:\n",
    "\n",
    "        seg_list = jieba.cut(document.strip())\n",
    "        fenci = []\n",
    "\n",
    "        for item in seg_list:\n",
    "            if item not in stoplist and re.match(r'-?\\d+\\.?\\d*', item) == None and len(item.strip()) > 0:\n",
    "                fenci.append(item)\n",
    "        # if the word segmentation of the sentence is null,the label of the sentence should be deleted accordingly\n",
    "        if len(fenci) > 0:\n",
    "            text_list.append(fenci)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(neg, pos):\n",
    "    neg_sege = wordsege(neg)\n",
    "    pos_sege = wordsege(pos)\n",
    "    combined = np.concatenate((pos_sege,neg_sege))\n",
    "    # generating label and meging label data\n",
    "    y = np.concatenate((np.ones(len(pos_sege), dtype=int), np.zeros(len(neg_sege), dtype=int)))\n",
    "    return combined,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\chenx\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.862 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "combined,y = tokenizer(neg, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(combined,y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(combined, size=128, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_size 指的是我们本身vector的size\n",
    "def transform_to_matrix(x, padding_size=256, vec_size=128):\n",
    "    res = []\n",
    "    for sen in x:\n",
    "        matrix = []\n",
    "        for i in range(padding_size):\n",
    "            try:\n",
    "                matrix.append(model[sen[i]].tolist())\n",
    "            except:\n",
    "                # 这里有两种except情况，\n",
    "                # 1. 这个单词找不到\n",
    "                # 2. sen没那么长\n",
    "                # 不管哪种情况，我们直接贴上全是0的vec\n",
    "                matrix.append([0] * vec_size)\n",
    "        res.append(matrix)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x_train = transform_to_matrix(x_train)\n",
    "x_test = transform_to_matrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5467, 256, 128)\n",
      "(2344, 256, 128)\n"
     ]
    }
   ],
   "source": [
    "# 搞成np的数组，便于处理\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# 看看数组的大小\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  \n",
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3, input_shape=(256, 128))`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, input_dim=128, input_length=256, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.6867430869222908\n",
      "Test accuracy: 0.564419795221843\n"
     ]
    }
   ],
   "source": [
    "# compile模型\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='mse',\n",
    "#               optimizer='adadelta',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# 两层LSTM\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=3, verbose=0)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 256, 128)          131584    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 526,081\n",
      "Trainable params: 526,081\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed \n",
    "from keras.layers import Bidirectional, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "input_shape = (256, 128) \n",
    "model.add(Bidirectional(LSTM(units=20,return_sequences=False),input_shape=input_shape))\n",
    "model.add(Dropout(0.5)) \n",
    "model.add(BatchNormalization()) \n",
    "# model.add(TimeDistributed(Dense(1, activation='sigmoid'))) \n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# LSTM参数个数计算：ht-1与xt拼接、隐藏单元数、四个门的bias \n",
    "# （20+40）*units*4+20*4 # # \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5467 samples, validate on 2344 samples\n",
      "Epoch 1/3\n",
      "5467/5467 [==============================] - 51s 9ms/step - loss: 0.6386 - acc: 0.6470 - val_loss: 0.5236 - val_acc: 0.7658\n",
      "Epoch 2/3\n",
      "5467/5467 [==============================] - 44s 8ms/step - loss: 0.4073 - acc: 0.8356 - val_loss: 0.3696 - val_acc: 0.8490\n",
      "Epoch 3/3\n",
      "5467/5467 [==============================] - 41s 7ms/step - loss: 0.3444 - acc: 0.8705 - val_loss: 0.3236 - val_acc: 0.8776\n",
      "2344/2344 [==============================] - 5s 2ms/step\n",
      "0.32361925889200727 0.8775597267590285\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=3, validation_data=(x_test, y_test), verbose=1)\n",
    "score, acc = model.evaluate(x_test, y_test,batch_size=batch_size)\n",
    "print (score, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_7 (Bidirection (None, 40)                23840     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 40)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 40)                160       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 41        \n",
      "=================================================================\n",
      "Total params: 24,041\n",
      "Trainable params: 23,961\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential() \n",
    "input_shape = (256, 128) \n",
    "model.add(Bidirectional(LSTM(units=20,return_sequences=True),input_shape=input_shape)) \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(BatchNormalization()) \n",
    "model.add(TimeDistributed(Dense(n_classes))) \n",
    "model.add(Dropout(0.2)) \n",
    "crf = ChainCRF() \n",
    "model.add(crf) \n",
    "model.compile(loss=crf.loss, optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "batch_size = 64 \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=30, validation_data=(x_test, y_test), verbose=1)\n",
    "score, acc = model.evaluate(x_test, y_test,batch_size=batch_size)\n",
    "print (score, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
