{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --*-- encoding:utf-8 --*--\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "def loadfile():\n",
    "    neg = pd.read_excel('./neg1.xls', header=None, index=None)\n",
    "    pos = pd.read_excel('./pos1.xls', header=None, index=None)\n",
    "    #merge all data\n",
    "    neg = np.array(neg[0])\n",
    "    pos = np.array(pos[0])\n",
    "    return neg,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = loadfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating set of disused words\n",
    "def getstopword(stopwordPath):\n",
    "    stoplist = set()\n",
    "    for line in stopwordPath:\n",
    "        stoplist.add(line.strip())\n",
    "        # print line.strip()\n",
    "    return stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divide the sentence and remove the disused words\n",
    "def wordsege(text):\n",
    "    # get disused words set\n",
    "    stopwordPath = open('./stopwords(ch).txt', 'r')\n",
    "    stoplist = getstopword(stopwordPath)\n",
    "    stopwordPath.close()\n",
    "\n",
    "    # divide the sentence and remove the disused words with jieba,return list\n",
    "    text_list = []\n",
    "    for document in text:\n",
    "\n",
    "        seg_list = jieba.cut(document.strip())\n",
    "        fenci = []\n",
    "\n",
    "        for item in seg_list:\n",
    "            if item not in stoplist and re.match(r'-?\\d+\\.?\\d*', item) == None and len(item.strip()) > 0:\n",
    "                fenci.append(item)\n",
    "        # if the word segmentation of the sentence is null,the label of the sentence should be deleted accordingly\n",
    "        if len(fenci) > 0:\n",
    "            text_list.append(fenci)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(neg, pos):\n",
    "    neg_sege = wordsege(neg)\n",
    "    pos_sege = wordsege(pos)\n",
    "    combined = np.concatenate((pos_sege,neg_sege))\n",
    "    # generating label and meging label data\n",
    "    y = np.concatenate((np.ones(len(pos_sege), dtype=int), np.zeros(len(neg_sege), dtype=int)))\n",
    "    return combined,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\chenx\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.615 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "combined,y = tokenizer(neg, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(combined,y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(combined, size=128, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec_size 指的是我们本身vector的size\n",
    "def transform_to_matrix(x, padding_size=256, vec_size=128):\n",
    "    res = []\n",
    "    for sen in x:\n",
    "        matrix = []\n",
    "        for i in range(padding_size):\n",
    "            try:\n",
    "                matrix.append(model[sen[i]].tolist())\n",
    "            except:\n",
    "                # 这里有两种except情况，\n",
    "                # 1. 这个单词找不到\n",
    "                # 2. sen没那么长\n",
    "                # 不管哪种情况，我们直接贴上全是0的vec\n",
    "                matrix.append([0] * vec_size)\n",
    "        res.append(matrix)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "x_train = transform_to_matrix(x_train)\n",
    "x_test = transform_to_matrix(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5467, 256, 128)\n",
      "(2344, 256, 128)\n"
     ]
    }
   ],
   "source": [
    "# 搞成np的数组，便于处理\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# 看看数组的大小\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5467, 1, 256, 128)\n",
      "(2344, 1, 256, 128)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 1, x_train.shape[1], x_train.shape[2])\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, x_test.shape[1], x_test.shape[2])\n",
    "#通过print(X_test)观察与前者的区别，就是多了一个括号\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (4, 4), input_shape=(1, 256, 1...)`\n",
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (4, 4))`\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed \n",
    "from keras.layers import Bidirectional, BatchNormalization\n",
    "\n",
    "# set parameters:\n",
    "batch_size = 32\n",
    "n_filter = 16\n",
    "filter_length = 4\n",
    "nb_epoch = 10\n",
    "n_pool = 2\n",
    "\n",
    "# 新建一个sequential的模型\n",
    "model = Sequential()\n",
    "model.add(Convolution2D(n_filter,filter_length,filter_length,\n",
    "                        input_shape=(1, 256, 128)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(n_filter,filter_length,filter_length))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(n_pool, n_pool)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5467 samples, validate on 2344 samples\n",
      "Epoch 1/3\n",
      "5467/5467 [==============================] - 618s 113ms/step - loss: 0.3482 - acc: 0.8688 - val_loss: 0.3073 - val_acc: 0.8823\n",
      "Epoch 2/3\n",
      "5467/5467 [==============================] - 877s 160ms/step - loss: 0.2424 - acc: 0.9109 - val_loss: 0.2285 - val_acc: 0.9160\n",
      "Epoch 3/3\n",
      "5467/5467 [==============================] - 1130s 207ms/step - loss: 0.1508 - acc: 0.9466 - val_loss: 0.1915 - val_acc: 0.9352\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23406f64860>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.add(Dense(1, activation='sigmoid'))\n",
    "batch_size = 64 \n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=3, validation_data=(x_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 16, 253, 125)      272       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 16, 253, 125)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 16, 250, 122)      4112      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 16, 250, 122)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 125, 61)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 125, 61)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 122000)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               15616128  \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 15,620,641\n",
      "Trainable params: 15,620,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.19146193599538186\n",
      "Test accuracy: 0.9351535836177475\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
