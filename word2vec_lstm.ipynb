{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import sequence\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import yaml\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.models import model_from_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337) # For Reproducibility\n",
    "# the dimension of word vector\n",
    "vocab_dim = 300\n",
    "# sentence length\n",
    "maxlen = 100\n",
    "# iter num\n",
    "n_iterations = 1\n",
    "# the number of words appearing\n",
    "n_exposures = 10\n",
    "# the maximum distance\n",
    "window_size = 7\n",
    "# batch size\n",
    "batch_size = 32\n",
    "# epoch num\n",
    "n_epoch = 20\n",
    "# input length\n",
    "input_length = 100\n",
    "# multi processing cpu number\n",
    "cpu_count = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "def loadfile():\n",
    "    neg = pd.read_excel('./neg.xls', header=None, index=None)\n",
    "    pos = pd.read_excel('./pos.xls', header=None, index=None)\n",
    "    #merge all data\n",
    "    neg = np.array(neg[0])\n",
    "    pos = np.array(pos[0])\n",
    "    return neg,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg, pos = loadfile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating set of disused words\n",
    "def getstopword(stopwordPath):\n",
    "    stoplist = set()\n",
    "    for line in stopwordPath:\n",
    "        stoplist.add(line.strip())\n",
    "        # print line.strip()\n",
    "    return stoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordsege(text):\n",
    "    stoplist = set()\n",
    "    stopwordPath = open('./stopwords(ch).txt', 'r')\n",
    "    for line in stopwordPath:\n",
    "        stoplist.add(line.strip())\n",
    "    stopwordPath.close()\n",
    "    \n",
    "    text_list = []\n",
    "    for document in text:\n",
    "        seg_list = jieba.cut(document.strip())\n",
    "        fenci = []\n",
    "        \n",
    "        for item in seg_list:\n",
    "            if item not in stoplist and re.match(r'-?\\d+\\.?\\d*', item)==None and len(item.strip())>0:\n",
    "                fenci.append(item)\n",
    "        # if the word segmentation of the sentence is null, the label of \n",
    "        # the sentence should be deleted accordingly\n",
    "        if len(fenci)>0:\n",
    "            text_list.append(fenci)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(neg, pos):\n",
    "    neg_sege = wordsege(neg)\n",
    "    pos_sege = wordsege(pos)\n",
    "    combined = np.concatenate((pos_sege,neg_sege))\n",
    "    # generating label and meging label data\n",
    "    y = np.concatenate((np.ones(len(pos_sege), dtype=int), np.zeros(len(neg_sege), dtype=int)))\n",
    "    return combined,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\chenx\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.703 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "combined,y = tokenizer(neg, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of words and phrases,return the index of each word,vector of words,and index of words corresponding to each sentence\n",
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        gensim_dict.doc2bow(model.wv.vocab.keys(),\n",
    "                            allow_update=True)\n",
    "        # the index of a word which have word vector is not 0\n",
    "        w2indx = {v: k + 1 for k, v in gensim_dict.items()}\n",
    "        # integrate all the corresponding word vectors into the word vector matrix\n",
    "        w2vec = {word: model[word] for word in w2indx.keys()}\n",
    "\n",
    "        # a word without a word vector is indexed 0,return the index of word\n",
    "        def parse_dataset(combined):\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data = []\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data\n",
    "\n",
    "        combined = parse_dataset(combined)\n",
    "        # unify the length of the sentence with the pad_sequences function of keras\n",
    "        combined = sequence.pad_sequences(combined, maxlen=maxlen)\n",
    "        # return index, word vector matrix and the sentence with an unifying length and indexed\n",
    "        return w2indx, w2vec, combined\n",
    "    else:\n",
    "        print('No data provided...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training of the word vector\n",
    "def word2vec_train(combined):\n",
    "    model = Word2Vec(size=vocab_dim,\n",
    "                     min_count=n_exposures,\n",
    "                     window=window_size,\n",
    "                     workers=cpu_count,\n",
    "                     iter=n_iterations)\n",
    "    # build the vocabulary dictionary\n",
    "    model.build_vocab(combined)\n",
    "    # train the word vector model\n",
    "    model.train(combined, total_examples=model.corpus_count, epochs=50)\n",
    "    # save the trained model\n",
    "    model.save('./Word2vec_model.pkl')\n",
    "    # index, word vector matrix and the sentence with an unifying length and indexed based on the trained model\n",
    "    index_dict, word_vectors, combined = create_dictionaries(model=model, combined=combined)\n",
    "\n",
    "    return index_dict, word_vectors, combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##定义网络结构\n",
    "def train_lstm(n_symbols, embedding_weights, x_train, y_train, x_test, y_test):\n",
    "    print('Defining a Simple Keras Model...')\n",
    "    model = Sequential()  # or Graph or whatever\n",
    "    model.add(Embedding(output_dim=vocab_dim,\n",
    "                        input_dim=n_symbols,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=input_length))  # Adding Input Length\n",
    "    model.add(LSTM(output_dim=50, activation='sigmoid', inner_activation='hard_sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print ('Compiling the Model...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    print (\"Train...\")\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, nb_epoch=n_epoch, verbose=1)\n",
    "\n",
    "    print (\"Evaluate...\")\n",
    "    score = model.evaluate(x_test, y_test,\n",
    "                           batch_size=batch_size)\n",
    "    # save the trained lstm model\n",
    "    yaml_string = model.to_yaml()\n",
    "    with open('./lstm.yml', 'w') as outfile:\n",
    "        outfile.write(yaml.dump(yaml_string, default_flow_style=True))\n",
    "    model.save_weights('./lstm.h5')\n",
    "    print ('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型，并保存\n",
    "def train():\n",
    "    print ('Loading Data...')\n",
    "    neg, post = loadfile()\n",
    "\n",
    "    print('Tokenising...')\n",
    "    combined,y = tokenizer(neg, post)\n",
    "    print(len(combined), len(y))\n",
    "    print('Training a Word2vec model...')\n",
    "    index_dict, word_vectors, combined = word2vec_train(combined)\n",
    "    print('Setting up Arrays for Keras Embedding Layer...')\n",
    "    n_symbols, embedding_weights, x_train, y_train, x_test, y_test = get_data(index_dict, word_vectors, combined, y)\n",
    "    print(x_train.shape, y_train.shape)\n",
    "    train_lstm(n_symbols, embedding_weights, x_train, y_train, x_test, y_test)\n",
    "\n",
    "# building the input format data\n",
    "def input_transform(string):\n",
    "    words = jieba.cut(string)\n",
    "    # reshape the list to bilayer list\n",
    "    words = np.array(words).reshape(1, -1)\n",
    "    model = Word2Vec.load('./Word2vec_model.pkl')\n",
    "    # create a dictionary of words and phrases,return the index of each word,vector of words,and index of words corresponding to each senten\n",
    "    _, _, combined = create_dictionaries(model, words)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def lstm_predict(string):\n",
    "    print('loading model......')\n",
    "    with open('./lstm.yml', 'r') as f:\n",
    "        yaml_string = yaml.load(f)\n",
    "    model = model_from_yaml(yaml_string)\n",
    "\n",
    "    print('loading weights......')\n",
    "    model.load_weights('./lstm.h5')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    data = input_transform(string)\n",
    "    data.reshape(1, -1)\n",
    "    # predict the new data\n",
    "    result = model.predict_classes(data)\n",
    "    if result[0][0] == 1:\n",
    "        print(string, ' positive')\n",
    "    else:\n",
    "        print(string, ' negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "Tokenising...\n",
      "21105 21105\n",
      "Training a Word2vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Arrays for Keras Embedding Layer...\n",
      "(16884, 100) (16884,)\n",
      "(16884, 100) (16884,)\n",
      "Defining a Simple Keras Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"sigmoid\", units=50, recurrent_activation=\"hard_sigmoid\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 300)          2381700   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,451,951\n",
      "Trainable params: 2,451,951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenx\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "16884/16884 [==============================] - 64s 4ms/step - loss: 0.4687 - acc: 0.7727\n",
      "Epoch 2/20\n",
      "16884/16884 [==============================] - 62s 4ms/step - loss: 0.2679 - acc: 0.9051\n",
      "Epoch 3/20\n",
      "16884/16884 [==============================] - 62s 4ms/step - loss: 0.2002 - acc: 0.9318\n",
      "Epoch 4/20\n",
      "16884/16884 [==============================] - 63s 4ms/step - loss: 0.1515 - acc: 0.9512\n",
      "Epoch 5/20\n",
      "16884/16884 [==============================] - 63s 4ms/step - loss: 0.1234 - acc: 0.9616\n",
      "Epoch 6/20\n",
      "16884/16884 [==============================] - 63s 4ms/step - loss: 0.1016 - acc: 0.9694\n",
      "Epoch 7/20\n",
      "16884/16884 [==============================] - 62s 4ms/step - loss: 0.0853 - acc: 0.9758\n",
      "Epoch 8/20\n",
      "16884/16884 [==============================] - 78s 5ms/step - loss: 0.0716 - acc: 0.9799: 1s - loss: 0.0714 - a\n",
      "Epoch 9/20\n",
      "16884/16884 [==============================] - 97s 6ms/step - loss: 0.0664 - acc: 0.9808: 0s - loss: 0.0665 - acc: 0.98\n",
      "Epoch 10/20\n",
      "16884/16884 [==============================] - 98s 6ms/step - loss: 0.0519 - acc: 0.9860\n",
      "Epoch 11/20\n",
      "16884/16884 [==============================] - 93s 6ms/step - loss: 0.0442 - acc: 0.9889\n",
      "Epoch 12/20\n",
      "16884/16884 [==============================] - 94s 6ms/step - loss: 0.0514 - acc: 0.9854\n",
      "Epoch 13/20\n",
      "16884/16884 [==============================] - 94s 6ms/step - loss: 0.0370 - acc: 0.9899\n",
      "Epoch 14/20\n",
      "16884/16884 [==============================] - 94s 6ms/step - loss: 0.0330 - acc: 0.9917\n",
      "Epoch 15/20\n",
      "16884/16884 [==============================] - 96s 6ms/step - loss: 0.0404 - acc: 0.9867\n",
      "Epoch 16/20\n",
      "16884/16884 [==============================] - 94s 6ms/step - loss: 0.0247 - acc: 0.9924: 4s - lo\n",
      "Epoch 17/20\n",
      "16884/16884 [==============================] - 98s 6ms/step - loss: 0.0236 - acc: 0.9942\n",
      "Epoch 18/20\n",
      "16884/16884 [==============================] - 98s 6ms/step - loss: 0.0197 - acc: 0.9945\n",
      "Epoch 19/20\n",
      "16884/16884 [==============================] - 96s 6ms/step - loss: 0.0280 - acc: 0.9918\n",
      "Epoch 20/20\n",
      "16884/16884 [==============================] - 87s 5ms/step - loss: 0.0245 - acc: 0.9933: 3s - los\n",
      "Evaluate...\n",
      "4221/4221 [==============================] - 5s 1ms/step\n",
      "Test score: [0.4241661518367369, 0.9121061361420637]\n",
      "loading model......\n",
      "loading weights......\n",
      "屏幕较差，拍照也很粗糙。  negative\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()\n",
    "    # string='电池充完了电连手机都打不开.简直烂的要命.真是金玉其外,败絮其中!连5号电池都不如'\n",
    "    # string='牛逼的手机，从3米高的地方摔下去都没坏，质量非常好'\n",
    "    # string='酒店的环境非常好，价格也便宜，值得推荐'\n",
    "    string='屏幕较差，拍照也很粗糙。'\n",
    "    # string='我是傻逼'\n",
    "    # string='你是傻逼'\n",
    "    # string = '屏幕较差，拍照也很粗糙。'\n",
    "    # string='质量不错，是正品 ，安装师傅也很好，才要了83元材料费'\n",
    "    # string='东西非常不错，安装师傅很负责人，装的也很漂亮，精致，谢谢安装师傅！'\n",
    "\n",
    "    lstm_predict(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
